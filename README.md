# Project: TinyLLaMA Reconstruction

**"From Tensors to TinyLLaMA"** íŒŒì´í† ì¹˜ ê¸°ì´ˆë¶€í„° LLM ì•„í‚¤í…ì²˜ê¹Œì§€, ë‹¨ê³„ë³„ë¡œ ì§ì ‘ êµ¬í˜„í•˜ë©° ëª¨ë¸ì˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ ë§ˆìŠ¤í„°í•˜ëŠ” êµìœ¡ìš© ë ˆí¬ì§€í† ë¦¬ì…ë‹ˆë‹¤.

---

### ğŸš€ Learning Roadmap

1.  **PyTorch Foundation** - í…ì„œ ì—°ì‚° ë§ˆìŠ¤í„°
2.  **NanoGPT (by Andrej Karpathy)** - Transformer ì•„í‚¤í…ì²˜ ë° Self-Attentionì˜ ê¸°ë³¸ êµ¬í˜„
3.  **llm.c (by Andrej Karpathy)** - C/CUDA ê´€ì ì—ì„œì˜ ì €ìˆ˜ì¤€ ìµœì í™” ë° GPT-2 êµ¬ì¡° í•™ìŠµ
4.  **TinyLLaMA Implementation** - Llama-2 ê¸°ë°˜ 1.1B íŒŒë¼ë¯¸í„° ëª¨ë¸ ì•„í‚¤í…ì²˜ ìµœì¢… êµ¬í˜„ (RoPE, RMSNorm, SwiGLU)

---

### ğŸ›  Tech Stack
- **Framework:** PyTorch, NumPy
- **Models:** NanoGPT, llm.c, TinyLLaMA
- **Environment:** MacBook Pro (MPS) & Kaggle (GPU)

---

### ğŸ”„ Workflow
```bash
# Pull from Kaggle
kaggle kernels pull howonmjeong/pratices -m

# Push to Kaggle
kaggle kernels push -p .
